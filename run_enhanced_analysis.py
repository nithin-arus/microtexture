#!/usr/bin/env python3
"""
Enhanced Texture Analysis Pipeline
==================================

Comprehensive example demonstrating all the new enhanced analysis capabilities:
- Sample-aware data splitting (prevents data leakage)
- Advanced feature selection
- Feature augmentation and data augmentation
- Deep learning integration
- Hyperparameter optimization
- Advanced visualizations (t-SNE, UMAP)
- Comprehensive benchmarking

Usage:
    python run_enhanced_analysis.py
"""

import os
import sys
import argparse
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from research.research_analysis import ResearchAnalyzer


def main():
    """Run comprehensive enhanced texture analysis."""
    
    parser = argparse.ArgumentParser(description='Enhanced Texture Analysis Pipeline')
    parser.add_argument('--data_path', type=str, default='data/features.csv',
                       help='Path to features CSV file')
    parser.add_argument('--output_dir', type=str, default='enhanced_analysis_output',
                       help='Output directory for results')
    parser.add_argument('--target_column', type=str, default='label',
                       help='Target column name for supervised learning')
    parser.add_argument('--quick_mode', action='store_true',
                       help='Run in quick mode (fewer trials for optimization)')
    parser.add_argument('--include_deep_learning', action='store_true',
                       help='Include deep learning analysis (requires raw images)')
    parser.add_argument('--skip_optimization', action='store_true',
                       help='Skip hyperparameter optimization to save time')
    
    args = parser.parse_args()
    
    print("="*60)
    print("ğŸš€ ENHANCED TEXTURE ANALYSIS PIPELINE")
    print("="*60)
    print("âœ¨ New Features:")
    print("  â€¢ Sample-aware splitting (prevents data leakage)")
    print("  â€¢ Advanced feature selection & augmentation")
    print("  â€¢ Deep learning CNN feature extraction")
    print("  â€¢ Hyperparameter optimization with Optuna")
    print("  â€¢ t-SNE/UMAP advanced visualizations")
    print("  â€¢ Comprehensive benchmarking suite")
    print("="*60)
    print()
    
    # Initialize analyzer
    analyzer = ResearchAnalyzer(
        data_path=args.data_path,
        output_dir=args.output_dir
    )
    
    try:
        # Step 1: Load and prepare data
        print("ğŸ“Š STEP 1: Loading and preparing data...")
        X, y, feature_names = analyzer.load_and_prepare_data(
            target_column=args.target_column,
            create_synthetic_labels=(args.target_column not in ['label'])
        )
        
        if X is None:
            print("âŒ No data loaded. Please check your data path.")
            return
        
        print(f"âœ… Loaded {X.shape[0]} samples with {X.shape[1]} features")
        print()
        
        # Step 2: Advanced feature selection
        print("ğŸ” STEP 2: Advanced feature selection...")
        feature_selection_results = analyzer.run_feature_selection_analysis(
            methods=['ensemble', 'mutual_information', 'stability_selection'],
            optimize_selection=(not args.skip_optimization)
        )
        print("âœ… Feature selection complete!")
        print()
        
        # Step 3: Enhanced supervised analysis with sample-aware splitting
        print("ğŸ¤– STEP 3: Enhanced supervised learning analysis...")
        print("ğŸ›¡ï¸  Using sample-aware splitting to prevent data leakage!")
        
        supervised_results = analyzer.run_supervised_analysis(
            test_size=0.2,
            val_size=0.1,
            cv_folds=5,
            use_sample_aware_splitting=True,
            apply_augmentation=True
        )
        
        print(f"âœ… Trained {len(supervised_results)} models with enhanced pipeline!")
        
        # Print best model performance
        best_model = max(supervised_results.items(), key=lambda x: x[1]['test_accuracy'])
        print(f"ğŸ† Best model: {best_model[0]} (Accuracy: {best_model[1]['test_accuracy']:.4f})")
        print()
        
        # Step 4: Deep learning analysis (if requested)
        if args.include_deep_learning:
            print("ğŸ§  STEP 4: Deep learning analysis...")
            deep_results = analyzer.run_deep_learning_analysis()
            hybrid_results = analyzer.run_hybrid_analysis()
            print("âœ… Deep learning analysis complete!")
            print()
        
        # Step 5: Advanced visualizations
        print("ğŸ“ˆ STEP 5: Advanced visualization suite...")
        viz_results = analyzer.run_advanced_visualization_suite()
        print("âœ… Advanced visualizations created!")
        print(f"   Generated: {len(viz_results)} visualization files")
        print()
        
        # Step 6: Comprehensive benchmarking
        print("ğŸ STEP 6: Comprehensive benchmarking...")
        print("   Comparing all approaches:")
        print("   1ï¸âƒ£  Baseline (random splitting)")
        print("   2ï¸âƒ£  Sample-aware splitting")
        print("   3ï¸âƒ£  Sample-aware + augmentation")
        print("   4ï¸âƒ£  Feature selection + sample-aware")
        
        benchmark_results = analyzer.run_comprehensive_benchmarking(
            include_augmentation=True,
            include_feature_selection=True,
            optimize_hyperparameters=(not args.skip_optimization)
        )
        
        print("âœ… Comprehensive benchmarking complete!")
        
        # Display benchmark results
        print("\nğŸ† BENCHMARK RESULTS:")
        for approach, results in benchmark_results.items():
            print(f"   {approach:20s}: {results['best_accuracy']:.4f} "
                  f"({results['feature_count']} features, "
                  f"leakage risk: {results['data_leakage_risk']})")
        
        best_approach = max(benchmark_results.items(), key=lambda x: x[1]['best_accuracy'])
        print(f"\nğŸ¥‡ WINNER: {best_approach[0]} "
              f"(Accuracy: {best_approach[1]['best_accuracy']:.4f})")
        print()
        
        # Step 7: Generate comprehensive report
        print("ğŸ“ STEP 7: Generating comprehensive report...")
        report_path = analyzer.generate_comprehensive_report()
        print(f"âœ… Report saved to: {report_path}")
        print()
        
        # Summary
        print("="*60)
        print("ğŸ‰ ENHANCED ANALYSIS COMPLETE!")
        print("="*60)
        print("ğŸ“Š Results Summary:")
        print(f"   â€¢ Best accuracy achieved: {best_approach[1]['best_accuracy']:.4f}")
        print(f"   â€¢ Best approach: {best_approach[1]['approach']}")
        print(f"   â€¢ Data leakage prevention: âœ… ENABLED")
        print(f"   â€¢ Feature augmentation: âœ… APPLIED")
        print(f"   â€¢ Advanced visualizations: âœ… CREATED")
        print()
        print("ğŸ“ Output files:")
        print(f"   â€¢ Main results: {args.output_dir}/")
        print(f"   â€¢ Visualizations: {args.output_dir}/visualizations/")
        print(f"   â€¢ Advanced plots: {args.output_dir}/advanced_visualizations/")
        print(f"   â€¢ Comprehensive report: {report_path}")
        print()
        print("ğŸš€ Performance improvements achieved:")
        
        if 'baseline' in benchmark_results and 'sample_aware' in benchmark_results:
            baseline_acc = benchmark_results['baseline']['best_accuracy']
            improved_acc = best_approach[1]['best_accuracy']
            improvement = ((improved_acc - baseline_acc) / baseline_acc) * 100
            print(f"   â€¢ Accuracy improvement: +{improvement:.1f}% vs baseline")
        
        print("   â€¢ Data leakage eliminated: âœ…")
        print("   â€¢ Feature space optimized: âœ…")
        print("   â€¢ Robustness enhanced: âœ…")
        print()
        print("ğŸ’¡ Key insights:")
        print("   â€¢ Sample-aware splitting prevents overly optimistic results")
        print("   â€¢ Feature augmentation helps with small datasets")
        print("   â€¢ Advanced feature selection reduces overfitting")
        print("   â€¢ Ensemble methods provide robust predictions")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ Error during analysis: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


def run_quick_demo():
    """Run a quick demonstration with default settings."""
    print("ğŸš€ Running quick demo of enhanced texture analysis...")
    
    # Check if data exists
    if not os.path.exists('data/features.csv'):
        print("âŒ No feature data found at 'data/features.csv'")
        print("ğŸ’¡ Please run the main data collection pipeline first:")
        print("   python main.py")
        return 1
    
    # Run analysis with default settings
    analyzer = ResearchAnalyzer(
        data_path='data/features.csv',
        output_dir='quick_demo_output'
    )
    
    # Load data
    X, y, feature_names = analyzer.load_and_prepare_data(target_column='label')
    
    if X is None:
        print("âŒ Could not load data")
        return 1
    
    print(f"âœ… Loaded {X.shape[0]} samples with {X.shape[1]} features")
    
    # Quick analysis
    print("ğŸ” Running sample-aware analysis (prevents data leakage)...")
    results = analyzer.run_supervised_analysis(
        use_sample_aware_splitting=True,
        apply_augmentation=False  # Skip augmentation for speed
    )
    
    # Best result
    best_model = max(results.items(), key=lambda x: x[1]['test_accuracy'])
    print(f"ğŸ† Best result: {best_model[0]} - {best_model[1]['test_accuracy']:.4f} accuracy")
    
    # Quick visualization
    analyzer.run_advanced_visualization_suite()
    
    print("âœ… Quick demo complete! Check 'quick_demo_output/' for results.")
    return 0


if __name__ == "__main__":
    # Check for quick demo mode
    if len(sys.argv) == 1 or (len(sys.argv) == 2 and sys.argv[1] == '--demo'):
        exit(run_quick_demo())
    else:
        exit(main()) 